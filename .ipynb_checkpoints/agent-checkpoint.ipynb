{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control with Deep Reinforcement Learning\n",
    "\n",
    "This is inspired by the DDPG paper from https://arxiv.org/abs/1509.02971. \n",
    "\n",
    "\n",
    "\n",
    "## OpenAI Bipedal Walker \n",
    "\n",
    "  This is simple 4-joints walker robot environment.\n",
    " \n",
    "  There are two versions:\n",
    " \n",
    "  - **Normal**, with slightly uneven terrain.\n",
    " \n",
    "  - **Hardcore** with ladders, stumps, pitfalls.\n",
    "  \n",
    "\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images/normal_env.png\"  style=\"width: 550px;\"/> </td>\n",
    "<td> <img src=\"images/hardcore_env.png\"  style=\"width: 550px;\"/> </td>\n",
    "</tr></table>\n",
    "  \n",
    "  \n",
    "  \n",
    "We are using the Normal environment to prototype the Deep Deterministic Policy Gradient (DDPG).\n",
    "\n",
    "\n",
    "#### Source\n",
    "\n",
    "  The BipedalEnvironment was created by Oleg Klimov and is licensed on the same terms as the rest of OpenAI Gym.  \n",
    "  Raw environment code: https://github.com/openai/gym/blob/master/gym/envs/box2d/bipedal_walker.py  \n",
    "\n",
    "\n",
    "### Rewards Given to the Agent\n",
    "\n",
    "  - Moving forward, total 300+ points up to the far end. \n",
    "  - If the robot falls, it gets -100. \n",
    "  - Applying motor torque costs a small amount of points, more optimal agent will get better score.\n",
    "\n",
    "### State Space: 24 Dimensions\n",
    "\n",
    "  - **4 hull measurements**: angle speed, angular velocity, horizontal speed, vertical speed\n",
    "  - **8 joint measurements**, 2 for each of the 4 joints: position of joints and joints angular speed \n",
    "  - **2 leg measurements**, one for each leg: legs contact with ground\n",
    "  - **10 lidar rangefinder measurements** to help to navigate the hardcore environment. \n",
    "  \n",
    "### What quantifies a solution, or a sucessful RL agent?\n",
    "  \n",
    "  To solve the game you need to get **300 points in 1600 time steps**.\n",
    " \n",
    "  To solve the hardcore version you need **300 points in 2000 time steps**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psuedocode / Planning\n",
    "\n",
    "Replay Buffer / Memory:\n",
    "    \n",
    "    state, action state_p, reward, d (terminal flag)\n",
    "\n",
    "\n",
    "2 Actor Networks; for Actor:\n",
    "\n",
    "    Chooses an action given a state\n",
    "    randomly sample states from memory\n",
    "    use actor to determine actions for those states\n",
    "    plug actions into critic and get the value\n",
    "    take gradient w.r.t the actor network params\n",
    "    backpropagate through the critic AND actor network for a given action\n",
    "    \n",
    "2 critic networks; for Critic:\n",
    "\n",
    "    evaluates value, by state and actions from actor network\n",
    "\n",
    "\n",
    "Polyak Averaging for Target Network Update where $\\theta^Q$ is the critic network weights\n",
    "\n",
    "$\\theta^{Q'} = \\rho\\theta^Q + (1-\\rho)\\theta^{Q'}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Chooses an action given a state\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions, d1_dims=256, d2_dims=256, action_range = (-1,1),\n",
    "                 save_dir='model_weights/', name = 'actor'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.actions = num_actions\n",
    "        self.d1_dims = d1_dims\n",
    "        self.d2_dims = d2_dims\n",
    "        self.action_range = action_range\n",
    "        \n",
    "        ## Create Model Checkpoint Destination\n",
    "        self.model_name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.file_checkpoint = os.path.join(self.save_dir, self.model_name +'.h5')\n",
    "        \n",
    "        \n",
    "        ## Build the Fully Connected Model Layers\n",
    "        \n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.1)\n",
    "        self.d1 = tf.keras.layers.Dense(self.d1_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(self.d2_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.action_vector = tf.keras.layers.Dense(self.actions, activation = 'tanh') #[-1,1]\n",
    "    \n",
    "    def call(self, state):\n",
    "        \n",
    "        \n",
    "        ## Forward propagation\n",
    "        mu = self.d1(state)\n",
    "        mu = self.d2(mu)\n",
    "        mu = self.action_vector(mu)\n",
    "        \n",
    "        ## Multiplay the tanh output by \n",
    "        mu = mu * max(self.action_range)\n",
    "        \n",
    "        return mu\n",
    "\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluates the value by the state and actions from the Actor Network\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d1_dims=256, d2_dims=256, save_dir='model_weights/', name = 'critic'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d1_dims = d1_dims\n",
    "        self.d2_dims = d2_dims\n",
    "        \n",
    "        \n",
    "        ## Create Model Checkpoint Destination\n",
    "        self.model_name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.file_checkpoint = os.path.join(self.save_dir, self.model_name +'.h5')\n",
    "        \n",
    "        \n",
    "        ## Build the Fully Connected Model Layers\n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.1)\n",
    "        self.d1 = tf.keras.layers.Dense(self.d1_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(self.d2_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.q = tf.keras.layers.Dense(1, activation = None)\n",
    "    \n",
    "    def call(self, state, action):\n",
    "        \n",
    "        ## Forward propagation\n",
    "        q = self.d1(tf.concat([state,action], axis=1))\n",
    "        q = self.d2(q)\n",
    "        q = self.q(q)\n",
    "        \n",
    "        return q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Chooses an action given a state\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions, d1_dims=256, d2_dims=256, action_range = (-1,1),\n",
    "                 save_dir='model_weights/', name = 'actor'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.actions = num_actions\n",
    "        self.d1_dims = d1_dims\n",
    "        self.d2_dims = d2_dims\n",
    "        self.action_range = action_range\n",
    "        \n",
    "        ## Create Model Checkpoint Destination\n",
    "        self.model_name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.file_checkpoint = os.path.join(self.save_dir, self.model_name +'.h5')\n",
    "        \n",
    "        \n",
    "        ## Build the Fully Connected Model Layers\n",
    "        \n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.1)\n",
    "        self.d1 = tf.keras.layers.Dense(self.d1_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(self.d2_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.d3 = tf.keras.layers.Dense(self.d1_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.action_vector = tf.keras.layers.Dense(self.actions, activation = 'tanh') #[-1,1]\n",
    "    \n",
    "    def call(self, state):\n",
    "        \n",
    "        \n",
    "        ## Forward propagation\n",
    "        mu = self.d1(state)\n",
    "        mu = self.d2(mu)\n",
    "        mu = self.d3(mu)\n",
    "        mu = self.action_vector(mu)\n",
    "        \n",
    "        ## Multiplay the tanh output by \n",
    "        mu = mu * max(self.action_range)\n",
    "        \n",
    "        return mu\n",
    "\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluates the value by the state and actions from the Actor Network\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d1_dims=256, d2_dims=256, save_dir='model_weights/', name = 'critic'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d1_dims = d1_dims\n",
    "        self.d2_dims = d2_dims\n",
    "        \n",
    "        \n",
    "        ## Create Model Checkpoint Destination\n",
    "        self.model_name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.file_checkpoint = os.path.join(self.save_dir, self.model_name +'.h5')\n",
    "        \n",
    "        \n",
    "        ## Build the Fully Connected Model Layers\n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=0.1)\n",
    "        self.d1 = tf.keras.layers.Dense(self.d1_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(self.d2_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.d3 = tf.keras.layers.Dense(self.d1_dims, kernel_initializer=initializer,\n",
    "                                                  activation='relu')\n",
    "        self.q = tf.keras.layers.Dense(1, activation = None)\n",
    "    \n",
    "    def call(self, state, action):\n",
    "        \n",
    "        ## Forward propagation\n",
    "        q = self.d1(tf.concat([state,action], axis=1))\n",
    "        q = self.d2(q)\n",
    "        q = self.d3(q)\n",
    "        q = self.q(q)\n",
    "        \n",
    "        return q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise():\n",
    "    \n",
    "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(\n",
    "                                                            self.mu, self.sigma)\n",
    "\n",
    "class ReplayBuffer():\n",
    "    \n",
    "    \"\"\"\n",
    "    Experience replay buffers stablize learning. This buffer\n",
    "    should be large enough to capture a wide range of experiences so that\n",
    "    it may generalize well.\n",
    "    \n",
    "    The buffer saves the (s, a, s', r, d) for each step in an environment.\n",
    "    The models will randomly sample experiences using a uniform distribution\n",
    "    later, to update the deep neural networks.\n",
    "    \n",
    "    Hyper-parameters:\n",
    "        memory_capacity - Too large and training is slow. Too small and\n",
    "            training will overfit to most recent experience\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, state_dims, num_actions):\n",
    "\n",
    "        self.memory_capacity = size\n",
    "        self.memory_index = 0\n",
    "        \n",
    "        ## Initialize a memory array for (s, a, s', r, d)\n",
    "        self.state_memory = np.zeros((self.memory_capacity, state_dims))\n",
    "        self.action_memory = np.zeros((self.memory_capacity, num_actions))\n",
    "        self.reward_memory = np.zeros(self.memory_capacity)\n",
    "        self.state_p_memory = np.zeros((self.memory_capacity, state_dims))\n",
    "        self.terminal_memory = np.zeros(self.memory_capacity, dtype=np.bool_)\n",
    "        \n",
    "            \n",
    "    def memorize(self, state, action, reward, state_p, terminal):\n",
    "        \n",
    "        ## Overwrite buffer when full\n",
    "        index = self.memory_index % self.memory_capacity\n",
    "        \n",
    "        ## Store Experience\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.state_p_memory[index] = state_p\n",
    "        self.terminal_memory[index] = terminal\n",
    "        \n",
    "        self.memory_index += 1\n",
    "    \n",
    "    def sample_memory(self, batch_size):\n",
    "         \n",
    "        sample_size = min(self.memory_index, self.memory_capacity)\n",
    "        \n",
    "        ## Randomly sample a batch of memories without replacement\n",
    "        batch = np.random.choice(sample_size, batch_size, replace=False)\n",
    "        \n",
    "        ## Generate Batch by Indices\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_p = self.state_p_memory[batch]\n",
    "        terminals = self.terminal_memory[batch]\n",
    "        \n",
    "        ## Numpy to Tensor\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        states_p = tf.convert_to_tensor(states_p, dtype=tf.float32)\n",
    "        #terminals = tf.convert_to_tensor(terminals, dtype=tf.float32)\n",
    "        \n",
    "        return states, actions, rewards, states_p, terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, env_name, lr_actor=0.001, lr_critic=0.002, env=None, gamma=0.95,\n",
    "                buffer_size = 20000, rho = 0.005, layer_dims =(400,300), batch_size=32,\n",
    "                epsilon=0.99, e_decay = 0.001):\n",
    "        self.save_dir = env_name + '_models'\n",
    "        self.num_actions = env.action_space.shape[0]\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.rho = rho\n",
    "        self.d1_dims = layer_dims[0]\n",
    "        self.d2_dims = layer_dims[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.e_decay = e_decay\n",
    "        self.noise = OUActionNoise(mu=np.zeros(self.num_actions))\n",
    "        self.action_range = (env.action_space.high[0], env.action_space.low[0])\n",
    "        self.action_counter = 0\n",
    "        \n",
    "        ## Define Replay Buffer\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.state_size, self.num_actions)\n",
    "        \n",
    "        ## Define Neural Networks\n",
    "        self.actor = Actor(save_dir = self.save_dir, num_actions = self.num_actions, \n",
    "                           d1_dims=self.d1_dims, d2_dims=self.d2_dims, \n",
    "                           action_range = self.action_range)\n",
    "        self.target_actor = Actor(save_dir = self.save_dir, num_actions = self.num_actions, \n",
    "                                  d1_dims=self.d1_dims, d2_dims=self.d2_dims, \n",
    "                                  action_range = self.action_range, \n",
    "                                  name = 'target_actor')\n",
    "        \n",
    "        self.critic = Critic(save_dir = self.save_dir, d1_dims=self.d1_dims, d2_dims=self.d2_dims)\n",
    "        self.target_critic = Critic(d1_dims=self.d1_dims, d2_dims=self.d2_dims, \n",
    "                                    name = 'target_critic', save_dir = self.save_dir,)\n",
    "        \n",
    "        ## Compile the Networks\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_actor))\n",
    "        self.target_actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_actor))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_critic))\n",
    "        self.target_critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_critic))\n",
    "        \n",
    "        ## Hard Copy Weights to Target Network\n",
    "        self.soft_update_weights(rho=1)\n",
    "        \n",
    "    def soft_update_weights(self, rho = None):\n",
    "        \"\"\"\n",
    "        Use polyak averaging as a soft weight update to the on-policy network\n",
    "        \n",
    "        https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
    "        \"\"\"\n",
    "        \n",
    "        if rho == None:\n",
    "            rho = self.rho\n",
    "        \n",
    "        ## Update Actor\n",
    "        weights = []\n",
    "        target_weights = self.target_actor.weights\n",
    "        for i, weight in enumerate(self.actor.weights):\n",
    "            weights.append(weight*rho + target_weights[i]*(1-rho))\n",
    "        self.target_actor.set_weights(weights)\n",
    "        \n",
    "        ## Update Critic\n",
    "        weights = []\n",
    "        target_weights = self.target_critic.weights\n",
    "        for i, weight in enumerate(self.critic.weights):\n",
    "            weights.append(weight*rho + target_weights[i]*(1-rho))\n",
    "        self.target_critic.set_weights(weights)\n",
    "        \n",
    "    def remember_experience(self, state, action, reward, state_p, terminal):\n",
    "        self.memory.memorize(state, action, reward, state_p, terminal)\n",
    "    \n",
    "    def act(self, state, greedy = False):\n",
    "        \n",
    "        ## E-Greedy Policy!\n",
    "        \n",
    "        self.action_counter+=1\n",
    "        \n",
    "        if greedy:\n",
    "            \n",
    "            return self.actor(tf.convert_to_tensor([state]), dtype='float32')[0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            epsilon = min(np.exp(-self.e_decay * self.action_counter), self.epsilon)\n",
    "            \n",
    "            if np.random.random() > epsilon:\n",
    "                return tf.convert_to_tensor(self.act_noisy(state))[0], 0\n",
    "            else:\n",
    "            \n",
    "                actions = tf.convert_to_tensor([self.env.action_space.sample()])\n",
    "                return actions[0], 1\n",
    "    \n",
    "    def choose_action(self, observation, evaluate=False):\n",
    "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "        actions = self.actor(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.num_actions],\n",
    "                    mean=0.0, stddev=0.1)\n",
    "        # note that if the environment has an action > 1, we have to multiply by\n",
    "        # max action at some point\n",
    "        actions = tf.clip_by_value(actions, min(self.action_range), max(self.action_range))\n",
    "\n",
    "        return actions[0]\n",
    "    \n",
    "    def act_noisy(self, state):\n",
    "        \n",
    "        action = self.actor(tf.convert_to_tensor([state], dtype='float32'))\n",
    "        action = action + self.noise()\n",
    "        action = np.clip(action, a_min = min(self.action_range), \n",
    "                         a_max = max(self.action_range))\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def save_weights(self, iteration = None):\n",
    "        \n",
    "        if iteration == None:\n",
    "            iteration = self.actor.save_dir\n",
    "        \n",
    "        print(f\"\\n........Initializing save at Episode {iteration}........\")\n",
    "        print(\"Saving Actor.....................\")\n",
    "        \n",
    "        if os.path.isdir(self.actor.save_dir) == False:\n",
    "            os.mkdir(self.actor.save_dir)\n",
    "        self.actor.save_weights(self.actor.file_checkpoint)\n",
    "        print(f\"Save Complete at {self.actor.file_checkpoint}\")\n",
    "        \n",
    "        print(\"Saving Critic.....................\")\n",
    "        if os.path.isdir(self.critic.save_dir) == False:\n",
    "            os.mkdir(self.critic.save_dir)\n",
    "        self.critic.save_weights(self.critic.file_checkpoint)\n",
    "        print(f\"Save Complete at {self.critic.file_checkpoint}\")\n",
    "        \n",
    "        print(\"Saving Target Networks.............\")\n",
    "        if os.path.isdir(self.target_actor.save_dir) == False:\n",
    "            os.mkdir(self.target_actor.save_dir)\n",
    "        if os.path.isdir(self.target_critic.save_dir) == False:\n",
    "            os.mkdir(self.target_critic.save_dir)\n",
    "        self.target_actor.save_weights(self.target_actor.file_checkpoint)\n",
    "        self.target_critic.save_weights(self.target_critic.file_checkpoint)\n",
    "        \n",
    "        print(\"Save Complete.\\n\")\n",
    "    \n",
    "    def load_weights(self):\n",
    "        \n",
    "        print(f\"........Loading Weights........\")\n",
    "        print(\"Loading Actor.....................\")\n",
    "        self.actor.load_weights(self.actor.file_checkpoint)\n",
    "        \n",
    "        print(\"Loading Critic.....................\")\n",
    "        self.critic.load_weights(self.critic.file_checkpoint)\n",
    "    \n",
    "        \n",
    "        print(\"Loading Target Networks.............\")\n",
    "        self.target_actor.load_weights(self.target_actor.file_checkpoint)\n",
    "        self.target_critic.load_weights(self.target_critic.file_checkpoint)\n",
    "        print(\"Load Complete.\")\n",
    "        \n",
    "    def learn(self):\n",
    "        \n",
    "        if self.memory.memory_index < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        ## Sample a Batch from Memory\n",
    "        states, actions, rewards, states_p, terminals = self.memory.sample_memory(self.batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            ## Feed s' to target_actor\n",
    "            mu_p = self.target_actor(states_p)\n",
    "            \n",
    "            # Feed s' and target_actor output to target_critic\n",
    "            q_p = tf.squeeze(self.target_critic(states_p, mu_p),1)\n",
    "            \n",
    "            ## Target is generated by target_critic, reward if terminal\n",
    "            target = rewards + self.gamma*q_p*(1-terminals)\n",
    "            \n",
    "            # Feed states and actions to crtic\n",
    "            q = tf.squeeze(self.critic(states,actions), 1)\n",
    "            \n",
    "            ## Critic \n",
    "            critic_loss = tf.keras.losses.MSE(q, target)\n",
    "            \n",
    "        ## Calculate the gradient of the loss w.r.t. the critic parameters\n",
    "        critic_gradient = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(critic_gradient, self.critic.trainable_variables))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            ## Actions of actor based on current weights\n",
    "            actor_actions = self.actor(states)\n",
    "            \n",
    "            ## Gradient ASCENT to MAXIMIZE Expected Value over time\n",
    "            actor_loss = tf.math.reduce_mean((-self.critic(states, actor_actions)))\n",
    "        \n",
    "        #\n",
    "        actor_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_gradient,self.actor.trainable_variables))\n",
    "        \n",
    "        self.soft_update_weights()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "........Initializing save at Episode 0........\n",
      "Saving Actor.....................\n",
      "Save Complete at LunarLanderContinuous-v2_models\\actor.h5\n",
      "Saving Critic.....................\n",
      "Save Complete at LunarLanderContinuous-v2_models\\critic.h5\n",
      "Saving Target Networks.............\n",
      "Save Complete.\n",
      "\n",
      "tf.Tensor([-0.5125351  -0.62160116], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  0 score -156.3 avg score -156.3 avg action  -0.4464166 over  59  actions\n",
      "episode  1 score -1428.3 avg score -792.3 avg action  0.9050902 over  114  actions\n",
      "episode  2 score -1644.7 avg score -1076.4 avg action  0.9597903 over  121  actions\n",
      "episode  3 score -1547.3 avg score -1194.2 avg action  0.9643386 over  122  actions\n",
      "episode  4 score -1737.2 avg score -1302.8 avg action  0.9568754 over  129  actions\n",
      "episode  5 score -748.7 avg score -1210.4 avg action  0.96457523 over  56  actions\n",
      "episode  6 score -783.1 avg score -1149.4 avg action  0.9506825 over  87  actions\n",
      "episode  7 score -805.1 avg score -1106.3 avg action  0.9628663 over  81  actions\n",
      "episode  8 score -1594.0 avg score -1160.5 avg action  0.96577126 over  118  actions\n",
      "episode  9 score -954.4 avg score -1139.9 avg action  0.95558006 over  95  actions\n",
      "episode  10 score -806.9 avg score -1109.6 avg action  0.95168364 over  66  actions\n",
      "episode  11 score -980.6 avg score -1098.9 avg action  0.9645321 over  83  actions\n",
      "episode  12 score -1706.8 avg score -1145.6 avg action  0.96251076 over  124  actions\n",
      "episode  13 score -1806.4 avg score -1192.8 avg action  0.9529798 over  136  actions\n",
      "episode  14 score -822.9 avg score -1168.2 avg action  0.9665411 over  67  actions\n",
      "episode  15 score -835.1 avg score -1147.4 avg action  0.96923417 over  74  actions\n",
      "episode  16 score -826.5 avg score -1128.5 avg action  0.965775 over  66  actions\n",
      "episode  17 score -1507.2 avg score -1149.5 avg action  0.96475714 over  112  actions\n",
      "episode  18 score -776.2 avg score -1129.9 avg action  0.9496414 over  59  actions\n",
      "episode  19 score -923.4 avg score -1119.6 avg action  0.95878416 over  90  actions\n",
      "tf.Tensor([1. 1.], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  20 score -843.9 avg score -1106.4 avg action  0.9688841 over  73  actions\n",
      "episode  21 score -796.1 avg score -1092.3 avg action  0.9582985 over  66  actions\n",
      "episode  22 score -839.4 avg score -1081.3 avg action  0.96240985 over  71  actions\n",
      "episode  23 score -790.4 avg score -1069.2 avg action  0.9669016 over  62  actions\n",
      "episode  24 score -2617.4 avg score -1131.1 avg action  0.9629644 over  233  actions\n",
      "episode  25 score -884.8 avg score -1121.7 avg action  0.95875955 over  79  actions\n",
      "episode  26 score -907.4 avg score -1113.7 avg action  0.9576383 over  77  actions\n",
      "episode  27 score -1031.7 avg score -1110.8 avg action  0.959924 over  95  actions\n",
      "episode  28 score -1832.0 avg score -1135.7 avg action  0.95711803 over  131  actions\n",
      "episode  29 score -826.6 avg score -1125.4 avg action  0.9603585 over  80  actions\n",
      "episode  30 score -724.3 avg score -1112.4 avg action  0.95693904 over  59  actions\n",
      "episode  31 score -984.2 avg score -1108.4 avg action  0.95943433 over  82  actions\n",
      "episode  32 score -827.2 avg score -1099.9 avg action  0.96224797 over  65  actions\n",
      "episode  33 score -1810.6 avg score -1120.8 avg action  0.9636025 over  131  actions\n",
      "episode  34 score -809.8 avg score -1111.9 avg action  0.9531182 over  66  actions\n",
      "episode  35 score -754.3 avg score -1102.0 avg action  0.9551228 over  57  actions\n",
      "episode  36 score -1664.7 avg score -1117.2 avg action  0.9581568 over  125  actions\n",
      "episode  37 score -976.9 avg score -1113.5 avg action  0.9533852 over  81  actions\n",
      "episode  38 score -1283.0 avg score -1117.8 avg action  0.95437807 over  98  actions\n",
      "episode  39 score -843.2 avg score -1111.0 avg action  0.9605649 over  72  actions\n",
      "tf.Tensor([1. 1.], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  40 score -1116.8 avg score -1111.1 avg action  0.9566986 over  96  actions\n",
      "episode  41 score -1057.4 avg score -1109.8 avg action  0.95430714 over  87  actions\n",
      "episode  42 score -716.1 avg score -1100.7 avg action  0.959162 over  60  actions\n",
      "episode  43 score -809.4 avg score -1094.1 avg action  0.9590657 over  67  actions\n",
      "episode  44 score -772.5 avg score -1086.9 avg action  0.95882475 over  59  actions\n",
      "episode  45 score -873.5 avg score -1082.3 avg action  0.9634843 over  74  actions\n",
      "episode  46 score -2374.8 avg score -1109.8 avg action  0.9594884 over  154  actions\n",
      "episode  47 score -1686.3 avg score -1121.8 avg action  0.9612352 over  125  actions\n",
      "episode  48 score -721.6 avg score -1113.6 avg action  0.9584636 over  59  actions\n",
      "episode  49 score -694.0 avg score -1105.2 avg action  0.9639965 over  56  actions\n",
      "episode  50 score -1863.1 avg score -1120.1 avg action  0.96055025 over  129  actions\n",
      "episode  51 score -860.2 avg score -1115.1 avg action  0.9588685 over  76  actions\n",
      "episode  52 score -1222.2 avg score -1117.1 avg action  0.95693254 over  98  actions\n",
      "episode  53 score -732.8 avg score -1110.0 avg action  0.96355075 over  59  actions\n",
      "episode  54 score -704.7 avg score -1102.6 avg action  0.96267474 over  56  actions\n",
      "episode  55 score -795.3 avg score -1097.1 avg action  0.96019244 over  87  actions\n",
      "episode  56 score -781.3 avg score -1091.6 avg action  0.961676 over  64  actions\n",
      "episode  57 score -1235.2 avg score -1094.1 avg action  0.9589445 over  103  actions\n",
      "episode  58 score -1066.8 avg score -1093.6 avg action  0.96542954 over  95  actions\n",
      "episode  59 score -695.5 avg score -1087.0 avg action  0.9616234 over  57  actions\n",
      "tf.Tensor([0.95280296 0.77332336], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  60 score -1275.2 avg score -1090.1 avg action  0.95434594 over  97  actions\n",
      "episode  61 score -857.5 avg score -1086.3 avg action  0.9564159 over  85  actions\n",
      "episode  62 score -904.2 avg score -1083.4 avg action  0.9592349 over  87  actions\n",
      "episode  63 score -743.4 avg score -1078.1 avg action  0.9585343 over  58  actions\n",
      "episode  64 score -760.3 avg score -1073.2 avg action  0.9684073 over  58  actions\n",
      "episode  65 score -1113.3 avg score -1073.8 avg action  0.9578934 over  90  actions\n",
      "episode  66 score -1658.8 avg score -1082.6 avg action  0.9552247 over  122  actions\n",
      "episode  67 score -831.4 avg score -1078.9 avg action  0.9657503 over  72  actions\n",
      "episode  68 score -1312.5 avg score -1082.2 avg action  0.964384 over  100  actions\n",
      "episode  69 score -1038.1 avg score -1081.6 avg action  0.96533394 over  85  actions\n",
      "episode  70 score -1900.4 avg score -1093.2 avg action  0.9546334 over  130  actions\n",
      "episode  71 score -891.9 avg score -1090.4 avg action  0.95693564 over  77  actions\n",
      "episode  72 score -822.8 avg score -1086.7 avg action  0.9561223 over  73  actions\n",
      "episode  73 score -735.1 avg score -1081.9 avg action  0.94595635 over  56  actions\n",
      "episode  74 score -787.0 avg score -1078.0 avg action  0.95723057 over  66  actions\n",
      "episode  75 score -773.7 avg score -1074.0 avg action  0.960683 over  61  actions\n",
      "episode  76 score -1717.8 avg score -1082.4 avg action  0.9610124 over  125  actions\n",
      "episode  77 score -1218.3 avg score -1084.1 avg action  0.9605101 over  96  actions\n",
      "episode  78 score -743.4 avg score -1079.8 avg action  0.9625207 over  57  actions\n",
      "episode  79 score -806.6 avg score -1076.4 avg action  0.9609467 over  68  actions\n",
      "tf.Tensor([1.        0.8822422], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  80 score -1005.3 avg score -1075.5 avg action  0.95394313 over  91  actions\n",
      "episode  81 score -1255.2 avg score -1077.7 avg action  0.957678 over  98  actions\n",
      "episode  82 score -1007.3 avg score -1076.8 avg action  0.9704871 over  83  actions\n",
      "episode  83 score -1566.3 avg score -1082.7 avg action  0.9629612 over  117  actions\n",
      "episode  84 score -925.5 avg score -1080.8 avg action  0.96195394 over  80  actions\n",
      "episode  85 score -1152.2 avg score -1081.7 avg action  0.96224785 over  93  actions\n",
      "episode  86 score -1413.4 avg score -1085.5 avg action  0.9585391 over  106  actions\n",
      "episode  87 score -894.0 avg score -1083.3 avg action  0.96025044 over  76  actions\n",
      "episode  88 score -943.7 avg score -1081.7 avg action  0.9670468 over  90  actions\n",
      "episode  89 score -1070.0 avg score -1081.6 avg action  0.96511877 over  89  actions\n",
      "episode  90 score -1403.4 avg score -1085.1 avg action  0.9615345 over  104  actions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  91 score -1156.1 avg score -1085.9 avg action  0.9598974 over  92  actions\n",
      "episode  92 score -739.0 avg score -1082.2 avg action  0.9565745 over  58  actions\n",
      "episode  93 score -1144.9 avg score -1082.8 avg action  0.95390505 over  96  actions\n",
      "episode  94 score -1130.5 avg score -1083.3 avg action  0.96446955 over  90  actions\n",
      "episode  95 score -764.2 avg score -1080.0 avg action  0.9586285 over  61  actions\n",
      "episode  96 score -1247.0 avg score -1081.7 avg action  0.9643491 over  105  actions\n",
      "episode  97 score -1246.7 avg score -1083.4 avg action  0.9674902 over  96  actions\n",
      "episode  98 score -1044.2 avg score -1083.0 avg action  0.969082 over  91  actions\n",
      "episode  99 score -2017.9 avg score -1092.4 avg action  0.95895165 over  137  actions\n",
      "tf.Tensor([0.939968 1.      ], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  100 score -1211.0 avg score -1102.9 avg action  0.95583266 over  95  actions\n",
      "episode  101 score -1176.5 avg score -1100.4 avg action  0.9587616 over  103  actions\n",
      "episode  102 score -1657.9 avg score -1100.5 avg action  0.9609697 over  121  actions\n",
      "episode  103 score -1915.9 avg score -1104.2 avg action  0.9571789 over  132  actions\n",
      "episode  104 score -1350.7 avg score -1100.4 avg action  0.9584764 over  114  actions\n",
      "episode  105 score -1297.7 avg score -1105.8 avg action  0.9546425 over  102  actions\n",
      "episode  106 score -954.0 avg score -1107.6 avg action  0.9606074 over  82  actions\n",
      "episode  107 score -1241.6 avg score -1111.9 avg action  0.9566466 over  96  actions\n",
      "episode  108 score -1503.1 avg score -1111.0 avg action  0.95233196 over  112  actions\n",
      "episode  109 score -1104.9 avg score -1112.5 avg action  0.9563322 over  96  actions\n",
      "episode  110 score -787.2 avg score -1112.3 avg action  0.9570731 over  65  actions\n",
      "episode  111 score -818.6 avg score -1110.7 avg action  0.96830815 over  71  actions\n",
      "episode  112 score -980.4 avg score -1103.4 avg action  0.9600352 over  84  actions\n",
      "episode  113 score -1301.9 avg score -1098.4 avg action  0.96274537 over  104  actions\n",
      "episode  114 score -1336.4 avg score -1103.5 avg action  0.9633698 over  114  actions\n",
      "episode  115 score -801.5 avg score -1103.2 avg action  0.9636527 over  69  actions\n",
      "episode  116 score -1086.9 avg score -1105.8 avg action  0.9629385 over  87  actions\n",
      "episode  117 score -1445.5 avg score -1105.2 avg action  0.96175295 over  108  actions\n",
      "episode  118 score -763.6 avg score -1105.0 avg action  0.9629048 over  58  actions\n",
      "episode  119 score -800.1 avg score -1103.8 avg action  0.9600343 over  64  actions\n",
      "tf.Tensor([0.99296165 0.8916692 ], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  120 score -731.8 avg score -1102.7 avg action  0.96062344 over  84  actions\n",
      "episode  121 score -714.0 avg score -1101.9 avg action  0.9624341 over  57  actions\n",
      "episode  122 score -1279.4 avg score -1106.3 avg action  0.9595023 over  96  actions\n",
      "episode  123 score -776.9 avg score -1106.1 avg action  0.958994 over  66  actions\n",
      "episode  124 score -1222.6 avg score -1092.2 avg action  0.96124035 over  104  actions\n",
      "episode  125 score -753.5 avg score -1090.9 avg action  0.9606641 over  61  actions\n",
      "episode  126 score -914.0 avg score -1090.9 avg action  0.9628814 over  81  actions\n",
      "episode  127 score -1055.2 avg score -1091.2 avg action  0.96002114 over  94  actions\n",
      "episode  128 score -874.8 avg score -1081.6 avg action  0.9597531 over  75  actions\n",
      "episode  129 score -850.3 avg score -1081.8 avg action  0.9582422 over  78  actions\n",
      "episode  130 score -680.9 avg score -1081.4 avg action  0.9649924 over  56  actions\n",
      "episode  131 score -846.5 avg score -1080.0 avg action  0.9623991 over  71  actions\n",
      "episode  132 score -1426.6 avg score -1086.0 avg action  0.956297 over  107  actions\n",
      "episode  133 score -2157.7 avg score -1089.5 avg action  0.9618056 over  145  actions\n",
      "episode  134 score -746.3 avg score -1088.9 avg action  0.9652201 over  61  actions\n",
      "episode  135 score -980.5 avg score -1091.1 avg action  0.96149397 over  85  actions\n",
      "episode  136 score -1199.2 avg score -1086.5 avg action  0.9646981 over  100  actions\n",
      "episode  137 score -761.5 avg score -1084.3 avg action  0.96184105 over  59  actions\n",
      "episode  138 score -787.1 avg score -1079.4 avg action  0.94676393 over  62  actions\n",
      "episode  139 score -817.3 avg score -1079.1 avg action  0.9623167 over  66  actions\n",
      "tf.Tensor([0.7897389 0.9631492], shape=(2,), dtype=float32)\n",
      "0\n",
      "episode  140 score -1092.8 avg score -1078.9 avg action  0.958944 over  89  actions\n",
      "episode  141 score -1540.2 avg score -1083.7 avg action  0.9530714 over  117  actions\n",
      "episode  142 score -947.6 avg score -1086.0 avg action  0.95686054 over  80  actions\n",
      "episode  143 score -738.8 avg score -1085.3 avg action  0.9689649 over  57  actions\n",
      "episode  144 score -1004.7 avg score -1087.6 avg action  0.9572673 over  84  actions\n",
      "episode  145 score -892.8 avg score -1087.8 avg action  0.9597011 over  76  actions\n",
      "episode  146 score -916.9 avg score -1073.2 avg action  0.95898515 over  81  actions\n",
      "episode  147 score -730.8 avg score -1063.7 avg action  0.9638251 over  63  actions\n",
      "episode  148 score -694.5 avg score -1063.4 avg action  0.9694972 over  55  actions\n",
      "episode  149 score -971.0 avg score -1066.2 avg action  0.95760864 over  84  actions\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #env = gym.make('BipedalWalker-v2')\n",
    "    env_name = 'LunarLanderContinuous-v2'\n",
    "    env = gym.make(env_name)\n",
    "    agent = Agent(env=env, env_name = env_name, layer_dims = (400,300), batch_size = 64, \n",
    "                  rho = 0.01, gamma = 0.95, lr_critic= 0.005, lr_actor= 0.003, \n",
    "                  epsilon = 0, e_decay = 0.000001)\n",
    "    \n",
    "    n_games = 2000\n",
    "\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "    load_checkpoint = False\n",
    "\n",
    "    if load_checkpoint:\n",
    "        n_steps = 0\n",
    "        while n_steps <= agent.batch_size:\n",
    "            observation = env.reset()\n",
    "            action = env.action_space.sample()\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            agent.remember_experience(observation, action, reward, observation_, done)\n",
    "            n_steps += 1\n",
    "        agent.learn()\n",
    "        agent.load_weights()\n",
    "        evaluate = False    ## Set true if you dont wnat to improve model\n",
    "    else:\n",
    "        evaluate = False\n",
    "\n",
    "        \n",
    "        \n",
    "    ## Game Loop\n",
    "    action_history = []\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        action_sequence = []\n",
    "        \n",
    "        while not done:\n",
    "            #action, explore = agent.act(observation)\n",
    "            #action_hist.append(explore)\n",
    "            action = agent.choose_action(observation)\n",
    "            action_sequence.append(action)\n",
    "            \n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.remember_experience(observation, action, reward, observation_, done)\n",
    "           \n",
    "            if not load_checkpoint:\n",
    "                agent.learn()\n",
    "            observation = observation_\n",
    "\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            if not load_checkpoint:\n",
    "                agent.save_weights(iteration = i)\n",
    "                \n",
    "        if i % 20 == 0:\n",
    "            print(action)\n",
    "            print(agent.action_counter)\n",
    "        action_history.append(action_sequence)\n",
    "        print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score, 'avg action ',\n",
    "              np.mean(action_sequence),'over ', len(action_sequence), ' actions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -297.8 avg score -125.6\n",
      "episode  1 score 3.9 avg score -124.7\n",
      "episode  2 score -189.4 avg score -125.6\n",
      "episode  3 score -202.9 avg score -126.4\n",
      "episode  4 score -204.0 avg score -127.4\n",
      "episode  5 score -105.0 avg score -126.9\n",
      "episode  6 score -96.8 avg score -126.8\n",
      "episode  7 score -203.9 avg score -127.5\n",
      "episode  8 score -162.8 avg score -127.6\n",
      "episode  9 score -139.2 avg score -127.2\n",
      "episode  10 score -149.5 avg score -127.9\n",
      "episode  11 score -57.3 avg score -128.1\n",
      "episode  12 score -84.3 avg score -128.4\n",
      "episode  13 score -124.9 avg score -125.2\n",
      "episode  14 score -60.6 avg score -124.3\n",
      "episode  15 score -95.9 avg score -124.4\n",
      "episode  16 score -110.9 avg score -124.9\n",
      "episode  17 score -89.9 avg score -125.1\n",
      "episode  18 score -177.8 avg score -124.7\n",
      "episode  19 score -97.7 avg score -120.6\n",
      "episode  20 score -116.4 avg score -117.5\n",
      "episode  21 score -100.5 avg score -117.3\n",
      "episode  22 score -112.5 avg score -117.7\n",
      "episode  23 score -335.8 avg score -120.5\n",
      "episode  24 score -111.6 avg score -121.1\n",
      "episode  25 score -142.2 avg score -121.3\n",
      "episode  26 score -99.5 avg score -121.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-125d447390a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m            \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m            \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-625c1d36fbce>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_gradient\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-625c1d36fbce>\u001b[0m in \u001b[0;36msoft_update_weights\u001b[1;34m(self, rho)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrho\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mremember_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1828\u001b[0m         \u001b[0mweight_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1830\u001b[1;33m     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1832\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3574\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3575\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3576\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3577\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3578\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.choose_action(observation, evaluate)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.remember_experience(observation, action, reward, observation_, done)\n",
    "            if not load_checkpoint:\n",
    "                agent.learn()\n",
    "            observation = observation_\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            #if not load_checkpoint:\n",
    "             #   agent.save_models()\n",
    "\n",
    "        print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b\n",
    "Should be able to change the update rule to be the mse between q and $G_t$ or other expected return method. Worth a shot, it'll give more weight to the rewards and contain less bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.linspace(0, 50000, 500), np.exp(-1/10000*np.linspace(0, 50000, 500)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-0.0001 * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(agent.action_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "for episode in action_history:\n",
    "    for item in episode:\n",
    "        actions.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(score_history)\n",
    "plt.plot(actions[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_hist = score_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
